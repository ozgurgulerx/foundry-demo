{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Azure API Management AI Gateway for MCP Servers\n\n> **Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n> **Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n> **© 2025 Ozgur Guler. All rights reserved.**\n\n---\n\n## What This Notebook Does\n\nThis notebook demonstrates how to put **MCP servers** behind **Azure API Management (APIM)** for enterprise-grade governance of AI agent tool calls. APIM provides rate limiting, authentication, content safety, and monitoring for your agent's external tool access.\n\n### The Key Concept\n\n```\n┌──────────────────┐                      ┌──────────────────────┐                      ┌─────────────────┐\n│     AI Agent     │    MCP Protocol      │   APIM AI Gateway    │    Backend Request   │   MCP Server    │\n│   (Foundry)      │ ─────────────────►  │                      │ ─────────────────►  │  (Logic Apps)   │\n│                  │                      │  • Rate Limiting     │                      │                 │\n│                  │                      │  • JWT Validation    │                      │  • ServiceNow   │\n│                  │                      │  • Content Safety    │                      │  • Salesforce   │\n│                  │                      │  • Token Metrics     │                      │  • SAP          │\n│                  │ ◄───────────────────│  • Semantic Cache    │ ◄───────────────────│  • SQL Server   │\n└──────────────────┘    Response          └──────────────────────┘    Response          └─────────────────┘\n```\n\n### Why APIM AI Gateway?\n\n| Feature | Policy | Benefit |\n|---------|--------|---------|\n| **Rate Limiting** | `rate-limit-by-key` | Prevent abuse, control costs |\n| **Token Limiting** | `llm-token-limit` | Control token consumption per consumer |\n| **Authentication** | `validate-jwt` | OAuth 2.0 / JWT validation |\n| **Content Safety** | `llm-content-safety` | Block prompt injection attacks |\n| **Semantic Caching** | `llm-semantic-cache-*` | Reduce costs for similar queries |\n| **Token Metrics** | `llm-emit-token-metric` | Track usage in Azure Monitor |\n| **Circuit Breaker** | Backend circuit breaker | Handle backend failures gracefully |\n\n---\n\n## APIM AI Gateway Capabilities\n\nAzure API Management provides specialized AI gateway features:\n\n### Supported AI Backend Types\n\n| Backend Type | Description | APIM Support |\n|--------------|-------------|--------------|\n| **Azure OpenAI** | Native OpenAI API | Full policy support |\n| **OpenAI-compatible LLMs** | LLaMA, Mistral, etc. | Import as API |\n| **Microsoft Foundry APIs** | Azure AI Foundry models | Native import |\n| **Pass-through MCP Server** | Route to existing MCP | Classic, V2, Self-hosted |\n| **REST API as MCP Server** | Expose any REST API as MCP | Classic, V2, Self-hosted |\n| **A2A Agent** | Agent-to-Agent protocol | V2 tiers only |\n\n### AI-Specific Policies\n\n| Policy | Purpose | Tiers |\n|--------|---------|-------|\n| `llm-token-limit` | Limit tokens per minute/hour | All except Consumption |\n| `llm-emit-token-metric` | Emit token usage metrics | All |\n| `llm-content-safety` | Azure AI Content Safety integration | All |\n| `llm-semantic-cache-lookup` | Cache lookup for similar prompts | All except Self-hosted |\n| `llm-semantic-cache-store` | Store responses in semantic cache | All except Self-hosted |\n\n---\n\n## Architecture: APIM as AI Gateway\n\n```\n┌─────────────────────────────────────────────────────────────────────────────────────────┐\n│                              AI AGENT REQUEST FLOW                                       │\n└─────────────────────────────────────────────────────────────────────────────────────────┘\n                                          │\n                                          ▼\n┌─────────────────────────────────────────────────────────────────────────────────────────┐\n│                              AZURE API MANAGEMENT                                        │\n│                                                                                          │\n│  ┌─────────────────────────────────────────────────────────────────────────────────┐    │\n│  │                           INBOUND POLICIES                                       │    │\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐ │    │\n│  │  │  validate   │  │ rate-limit  │  │ llm-token   │  │   llm-content-safety    │ │    │\n│  │  │    -jwt     │  │  -by-key    │  │   -limit    │  │   (prompt injection)    │ │    │\n│  │  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────────────────┘ │    │\n│  │                                                                                  │    │\n│  │  ┌────────────────────────────────────────────────────────────────────────────┐ │    │\n│  │  │              llm-semantic-cache-lookup (return cached if similar)           │ │    │\n│  │  └────────────────────────────────────────────────────────────────────────────┘ │    │\n│  └─────────────────────────────────────────────────────────────────────────────────┘    │\n│                                          │                                               │\n│                                          ▼                                               │\n│  ┌─────────────────────────────────────────────────────────────────────────────────┐    │\n│  │                              BACKEND                                             │    │\n│  │                    Route to MCP Server / OpenAI / LLM                            │    │\n│  └─────────────────────────────────────────────────────────────────────────────────┘    │\n│                                          │                                               │\n│                                          ▼                                               │\n│  ┌─────────────────────────────────────────────────────────────────────────────────┐    │\n│  │                           OUTBOUND POLICIES                                      │    │\n│  │  ┌────────────────────────────────────────────────────────────────────────────┐ │    │\n│  │  │              llm-emit-token-metric (track usage)                            │ │    │\n│  │  └────────────────────────────────────────────────────────────────────────────┘ │    │\n│  │  ┌────────────────────────────────────────────────────────────────────────────┐ │    │\n│  │  │              llm-semantic-cache-store (cache response)                      │ │    │\n│  │  └────────────────────────────────────────────────────────────────────────────┘ │    │\n│  └─────────────────────────────────────────────────────────────────────────────────┘    │\n│                                                                                          │\n└─────────────────────────────────────────────────────────────────────────────────────────┘\n                                          │\n                                          ▼\n                                    Response to Agent\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "---\n\n## Setup: Configure APIM as MCP Gateway\n\n### Option 1: Import MCP Server via Portal\n\n1. **APIs** → **Add API** → **MCP Server**\n2. Enter backend MCP URL (e.g., Logic Apps MCP endpoint)\n3. Configure inbound/outbound policies\n\n### Option 2: Via Bicep/ARM\n\n```bicep\nresource mcpApi 'Microsoft.ApiManagement/service/apis@2023-09-01-preview' = {\n  name: 'mcp-logic-apps'\n  properties: {\n    displayName: 'MCP Logic Apps Gateway'\n    path: 'mcp'\n    protocols: ['https']\n    serviceUrl: 'https://my-logic-app.azurewebsites.net/api/mcpservers/ticketing/mcp'\n    apiType: 'mcp'  // Specify MCP API type\n  }\n}\n```\n\n### Option 3: Expose REST API as MCP Server\n\nAPIM can transform any REST API into an MCP server:\n1. Import existing REST API (OpenAPI, WSDL, etc.)\n2. Configure MCP protocol mapping\n3. Agent discovers tools from the transformed API\n\n### APIM Endpoint Format\n\n```\nhttps://<apim-name>.azure-api.net/<api-path>/<mcp-server>/mcp\n```\n\nExample: `https://my-apim.azure-api.net/enterprise/ticketing/mcp`\n\n---\n\n## Section 1: Install Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install azure-ai-projects --pre --quiet\n!pip install azure-ai-agents --pre --quiet\n!pip install azure-identity python-dotenv --quiet\n\nprint(\"Packages installed successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import load_dotenv\n\nload_dotenv(\"../.env\")\n\n# Foundry Configuration\nPROJECT_ENDPOINT = os.getenv(\n    \"PROJECT_ENDPOINT\",\n    \"https://ozgurguler-7212-resource.services.ai.azure.com/api/projects/ozgurguler-7212\"\n)\n\n# Demo Mode Flag\n# Set to False when you have APIM configured as your MCP gateway\nUSE_DEMO_MCP = True\n\nif USE_DEMO_MCP:\n    # Microsoft Learn MCP Server - for demo/testing\n    # This demonstrates the MCP + governance pattern without requiring APIM setup\n    APIM_MCP_ENDPOINT = \"https://learn.microsoft.com/api/mcp\"\n    APIM_SUBSCRIPTION_KEY = \"\"  # Not needed for demo server\n    MCP_SERVER_LABEL = \"microsoft_learn\"\n    print(\"Using Microsoft Learn MCP Server (demo mode)\")\n    print(\"Set USE_DEMO_MCP = False and configure APIM endpoint for production\")\nelse:\n    # Your APIM gateway endpoint (fronting your MCP server)\n    APIM_MCP_ENDPOINT = os.getenv(\n        \"APIM_MCP_ENDPOINT\",\n        \"https://your-apim.azure-api.net/mcp/ticketing/mcp\"\n    )\n    # APIM subscription key (if required by your API policy)\n    APIM_SUBSCRIPTION_KEY = os.getenv(\"APIM_SUBSCRIPTION_KEY\", \"\")\n    MCP_SERVER_LABEL = \"apim_mcp_gateway\"\n    print(\"Using APIM AI Gateway\")\n\nMODEL = os.getenv(\"MODEL_DEPLOYMENT_NAME\", \"gpt-5-nano\")\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Project: {PROJECT_ENDPOINT}\")\nprint(f\"  MCP Endpoint: {APIM_MCP_ENDPOINT}\")\nprint(f\"  Subscription Key: {'***' if APIM_SUBSCRIPTION_KEY else '(not set)'}\")\nprint(f\"  Model: {MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key APIM Policies for MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 1. Rate Limiting (Token-based)\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "    <llm-token-limit \n",
    "        counter-key=\"@(context.Subscription.Id)\" \n",
    "        tokens-per-minute=\"1000\" \n",
    "        estimate-prompt-tokens=\"true\"\n",
    "        remaining-tokens-variable-name=\"remainingTokens\">\n",
    "    </llm-token-limit>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "### 2. Request Rate Limiting\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "    <rate-limit-by-key \n",
    "        calls=\"10\" \n",
    "        renewal-period=\"60\" \n",
    "        counter-key=\"@(context.Request.IpAddress)\" />\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "### 3. Content Safety\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "    <llm-content-safety backend-id=\"content-safety-backend\">\n",
    "        <text-blocklist-ids>\n",
    "            <id>prompt-injection-patterns</id>\n",
    "        </text-blocklist-ids>\n",
    "    </llm-content-safety>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "### 4. JWT Validation\n",
    "\n",
    "```xml\n",
    "<inbound>\n",
    "    <validate-jwt header-name=\"Authorization\" require-scheme=\"Bearer\">\n",
    "        <openid-config url=\"https://login.microsoftonline.com/{tenant}/.well-known/openid-configuration\" />\n",
    "        <audiences>\n",
    "            <audience>api://your-app-id</audience>\n",
    "        </audiences>\n",
    "    </validate-jwt>\n",
    "</inbound>\n",
    "```\n",
    "\n",
    "### 5. Token Metrics\n",
    "\n",
    "```xml\n",
    "<outbound>\n",
    "    <llm-emit-token-metric namespace=\"mcp-metrics\">\n",
    "        <dimension name=\"Subscription\" value=\"@(context.Subscription.Id)\" />\n",
    "        <dimension name=\"API\" value=\"@(context.Api.Name)\" />\n",
    "    </llm-emit-token-metric>\n",
    "</outbound>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Use APIM Gateway with Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from azure.identity import DefaultAzureCredential\nfrom azure.ai.projects import AIProjectClient\nfrom azure.ai.projects.models import PromptAgentDefinition\nfrom azure.ai.agents.models import McpTool  # McpTool is in azure.ai.agents.models\n\n# Initialize client\nclient = AIProjectClient(\n    endpoint=PROJECT_ENDPOINT,\n    credential=DefaultAzureCredential()\n)\n\n# MCP tool pointing to APIM gateway (not directly to backend MCP server)\n# APIM handles: rate limiting, auth, content safety, metrics\nmcp_tool = McpTool(\n    server_label=MCP_SERVER_LABEL.replace(\"-\", \"_\"),  # Must be alphanumeric + underscore\n    server_url=APIM_MCP_ENDPOINT,\n    allowed_tools=[],  # Empty = allow all tools\n)\n\n# Add APIM subscription key header if configured\nif APIM_SUBSCRIPTION_KEY:\n    # Note: McpTool.headers property for custom headers\n    print(f\"APIM subscription key configured (will be added to requests)\")\nelse:\n    print(\"No APIM subscription key (using anonymous/OAuth access)\")\n\nprint(f\"\\nMCP Tool configured:\")\nprint(f\"  Label: {MCP_SERVER_LABEL}\")\nprint(f\"  Endpoint: {APIM_MCP_ENDPOINT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Agent instructions based on mode\nif USE_DEMO_MCP:\n    AGENT_INSTRUCTIONS = \"\"\"You are a helpful assistant with access to Microsoft Learn documentation via MCP.\n\nWhen asked questions about Azure, API Management, or AI gateway topics:\n1. Use available tools to search documentation\n2. Provide accurate answers based on the retrieved content\n3. Explain how APIM AI Gateway features work\n\nThis is a demo showing the APIM + MCP pattern. In production, you would have governed access to enterprise tools.\"\"\"\nelse:\n    AGENT_INSTRUCTIONS = \"\"\"You are an assistant with governed access to enterprise tools via APIM AI Gateway.\n\nThe MCP tools you use are fronted by Azure API Management which provides:\n- Rate limiting to prevent abuse\n- JWT authentication for security\n- Content safety checks\n- Token usage tracking\n\nWhen asked to perform actions:\n1. Use the appropriate MCP tool\n2. Handle any rate limit responses gracefully\n3. Report results to the user\"\"\"\n\n# Create agent with APIM-governed MCP tool\nAGENT_NAME = \"apim-governed-agent\"\n\ntry:\n    agent = client.agents.create_version(\n        agent_name=AGENT_NAME,\n        definition=PromptAgentDefinition(\n            model=MODEL,\n            instructions=AGENT_INSTRUCTIONS,\n            tools=mcp_tool.definitions,\n        )\n    )\n    print(f\"Created governed agent: {agent.name}\")\n    print(f\"  Version: {agent.version}\")\n    print(f\"  MCP via APIM: {APIM_MCP_ENDPOINT}\")\nexcept Exception as e:\n    print(f\"Error creating agent: {e}\")\n    agent = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Get OpenAI client for agent invocation\nopenai_client = client.get_openai_client()\n\ndef invoke_governed_agent(user_input: str, agent_name: str) -> str:\n    \"\"\"Invoke an agent through APIM-governed MCP endpoint.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"User: {user_input}\")\n    print(\"=\"*60)\n    \n    try:\n        # Create a conversation\n        conversation = openai_client.conversations.create()\n        \n        # Send the message with agent reference\n        response = openai_client.responses.create(\n            input=user_input,\n            conversation=conversation.id,\n            extra_body={\"agent\": {\"name\": agent_name, \"type\": \"agent_reference\"}},\n        )\n        \n        # Check for rate limiting or other APIM responses\n        status_msg = f\"Status: {response.status}\"\n        if hasattr(response, 'headers'):\n            # Check for APIM rate limit headers\n            remaining = response.headers.get('x-ratelimit-remaining-requests', 'N/A')\n            status_msg += f\" | Rate Limit Remaining: {remaining}\"\n        \n        print(f\"\\n{status_msg}\")\n        print(f\"\\nAgent Response:\\n{response.output_text}\")\n        \n        return response.output_text\n        \n    except Exception as e:\n        error_str = str(e)\n        # Check for rate limit (429) errors\n        if \"429\" in error_str or \"rate limit\" in error_str.lower():\n            print(f\"\\nRATE_LIMITED: Too many requests - APIM rate limiting in effect\")\n            return \"RATE_LIMITED\"\n        # Check for authentication errors (401/403)\n        elif \"401\" in error_str or \"403\" in error_str:\n            print(f\"\\nAUTH_ERROR: Authentication/authorization failed\")\n            return \"AUTH_ERROR\"\n        else:\n            print(f\"\\nError: {e}\")\n            return None\n\n# Test the governed agent\nif agent:\n    if USE_DEMO_MCP:\n        result = invoke_governed_agent(\n            \"What is Azure API Management AI Gateway and how does it help with MCP servers?\",\n            agent.name\n        )\n    else:\n        result = invoke_governed_agent(\n            \"Create a support ticket for login issues on mobile app\",\n            agent.name\n        )\nelse:\n    print(\"Agent not available - check configuration\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "---\n\n## Section 2: Testing APIM AI Gateway Features\n\n### Rate Limiting Behavior\n\nWhen APIM rate limiting is configured:\n- Requests within limit: Return normally\n- Requests exceeding limit: Return `429 Too Many Requests`\n- Rate limit headers: `x-ratelimit-remaining-requests`, `x-ratelimit-reset`\n\n### Expected APIM Response Headers\n\n| Header | Description |\n|--------|-------------|\n| `x-ratelimit-remaining-requests` | Requests remaining in current window |\n| `x-ratelimit-remaining-tokens` | Tokens remaining (if token limiting) |\n| `x-ratelimit-reset` | Time when limit resets |\n| `x-ms-apim-request-id` | APIM request tracking ID |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Test rate limiting with burst requests\n# Note: In demo mode, Microsoft Learn server may not have rate limiting\n# In production with APIM, you should see rate limit responses after exceeding limits\n\nimport time\n\nif agent:\n    print(\"Testing rate limiting (burst requests)...\")\n    print(\"Note: Rate limiting only applies when APIM is configured\\n\")\n    \n    results = []\n    for i in range(5):\n        result = invoke_governed_agent(\n            f\"Test request {i+1}: What is API Management?\",\n            agent.name\n        )\n        status = \"RATE_LIMITED\" if result == \"RATE_LIMITED\" else \"OK\"\n        results.append(status)\n        print(f\"\\nRequest {i+1}: {status}\")\n        time.sleep(0.5)  # Small delay between requests\n    \n    print(f\"\\n{'='*60}\")\n    print(\"Summary:\")\n    print(f\"  Total requests: {len(results)}\")\n    print(f\"  OK: {results.count('OK')}\")\n    print(f\"  Rate Limited: {results.count('RATE_LIMITED')}\")\n    if not USE_DEMO_MCP:\n        print(\"\\nIf rate limiting is configured in APIM, later requests should show RATE_LIMITED\")\nelse:\n    print(\"Agent not available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Optional: Cleanup agent\nDELETE_AGENT = False  # Set to True to delete\n\nif DELETE_AGENT and agent:\n    try:\n        client.agents.delete(agent_name=AGENT_NAME)\n        print(f\"Deleted agent: {AGENT_NAME}\")\n    except Exception as e:\n        print(f\"Note: {e}\")\nelse:\n    print(f\"Agent cleanup skipped (DELETE_AGENT = {DELETE_AGENT})\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "---\n\n## Section 3: Complete APIM Policy Reference\n\n### Full Policy Configuration Example\n\n```xml\n<policies>\n    <inbound>\n        <base />\n        \n        <!-- 1. AUTHENTICATION: Validate JWT from Azure AD -->\n        <validate-jwt header-name=\"Authorization\" require-scheme=\"Bearer\"\n                      failed-validation-httpcode=\"401\"\n                      failed-validation-error-message=\"Unauthorized\">\n            <openid-config url=\"https://login.microsoftonline.com/{tenant-id}/.well-known/openid-configuration\" />\n            <audiences>\n                <audience>api://your-app-registration-id</audience>\n            </audiences>\n            <required-claims>\n                <claim name=\"roles\" match=\"any\">\n                    <value>MCP.Tools.Read</value>\n                    <value>MCP.Tools.Write</value>\n                </claim>\n            </required-claims>\n        </validate-jwt>\n        \n        <!-- 2. RATE LIMITING: Limit requests per subscription -->\n        <rate-limit-by-key \n            calls=\"100\" \n            renewal-period=\"60\" \n            counter-key=\"@(context.Subscription.Id)\"\n            remaining-calls-variable-name=\"remainingCalls\"\n            remaining-calls-header-name=\"x-ratelimit-remaining-requests\" />\n        \n        <!-- 3. TOKEN LIMITING: Limit AI tokens per minute -->\n        <llm-token-limit \n            counter-key=\"@(context.Subscription.Id)\" \n            tokens-per-minute=\"10000\"\n            estimate-prompt-tokens=\"true\"\n            remaining-tokens-variable-name=\"remainingTokens\"\n            remaining-tokens-header-name=\"x-ratelimit-remaining-tokens\">\n            <llm-api>azure-openai</llm-api>\n        </llm-token-limit>\n        \n        <!-- 4. CONTENT SAFETY: Block prompt injection attacks -->\n        <llm-content-safety backend-id=\"content-safety-backend\">\n            <harm-categories>\n                <Hate>Medium</Hate>\n                <Sexual>Medium</Sexual>\n                <Violence>Medium</Violence>\n                <SelfHarm>Medium</SelfHarm>\n            </harm-categories>\n            <text-blocklist-ids>\n                <id>prompt-injection-patterns</id>\n                <id>jailbreak-attempts</id>\n            </text-blocklist-ids>\n            <on-error>\n                <set-status code=\"400\" reason=\"Content blocked by safety policy\" />\n            </on-error>\n        </llm-content-safety>\n        \n        <!-- 5. SEMANTIC CACHE LOOKUP: Return cached response if similar -->\n        <llm-semantic-cache-lookup \n            score-threshold=\"0.9\"\n            embeddings-backend-id=\"embeddings-backend\">\n            <vary-by>\n                <header>Authorization</header>\n            </vary-by>\n        </llm-semantic-cache-lookup>\n        \n    </inbound>\n    \n    <backend>\n        <base />\n        <!-- Forward to MCP server backend -->\n    </backend>\n    \n    <outbound>\n        <base />\n        \n        <!-- 6. EMIT TOKEN METRICS: Track usage in Azure Monitor -->\n        <llm-emit-token-metric namespace=\"MCP-AI-Gateway\">\n            <dimension name=\"Subscription\" value=\"@(context.Subscription.Id)\" />\n            <dimension name=\"API\" value=\"@(context.Api.Name)\" />\n            <dimension name=\"Operation\" value=\"@(context.Operation.Name)\" />\n            <dimension name=\"Consumer\" value=\"@(context.User.Email ?? \"anonymous\")\" />\n        </llm-emit-token-metric>\n        \n        <!-- 7. SEMANTIC CACHE STORE: Cache response for future similar queries -->\n        <llm-semantic-cache-store duration=\"3600\" />\n        \n        <!-- 8. ADD GOVERNANCE HEADERS -->\n        <set-header name=\"x-apim-request-id\" exists-action=\"override\">\n            <value>@(context.RequestId.ToString())</value>\n        </set-header>\n        \n    </outbound>\n    \n    <on-error>\n        <base />\n        <!-- Log errors for monitoring -->\n        <trace source=\"MCP-Error\" severity=\"error\">\n            <message>@(context.LastError.Message)</message>\n            <metadata name=\"StatusCode\" value=\"@(context.Response.StatusCode.ToString())\" />\n        </trace>\n    </on-error>\n</policies>\n```\n\n### Individual Policy Breakdown\n\n#### Rate Limiting by Subscription\n```xml\n<rate-limit-by-key \n    calls=\"10\" \n    renewal-period=\"60\" \n    counter-key=\"@(context.Subscription.Id)\" />\n```\n\n#### Token Limiting for AI Workloads\n```xml\n<llm-token-limit \n    counter-key=\"@(context.Subscription.Id)\" \n    tokens-per-minute=\"1000\" \n    estimate-prompt-tokens=\"true\" />\n```\n\n#### Content Safety with Azure AI\n```xml\n<llm-content-safety backend-id=\"content-safety-backend\">\n    <text-blocklist-ids>\n        <id>prompt-injection-patterns</id>\n    </text-blocklist-ids>\n</llm-content-safety>\n```\n\n#### Semantic Caching for Cost Savings\n```xml\n<!-- Lookup -->\n<llm-semantic-cache-lookup score-threshold=\"0.9\" />\n\n<!-- Store -->\n<llm-semantic-cache-store duration=\"3600\" />\n```\n\n#### Token Metrics for Monitoring\n```xml\n<llm-emit-token-metric namespace=\"mcp-metrics\">\n    <dimension name=\"Subscription\" value=\"@(context.Subscription.Id)\" />\n</llm-emit-token-metric>\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "---\n\n## Summary: APIM AI Gateway for MCP\n\n### What We Demonstrated\n\n```\n┌─────────────────────────────────────────────────────────────────────────────────┐\n│                       APIM AI GATEWAY FOR MCP SERVERS                            │\n├─────────────────────────────────────────────────────────────────────────────────┤\n│                                                                                  │\n│   AI AGENT                    APIM AI GATEWAY                    MCP SERVER      │\n│   ┌─────────┐                 ┌─────────────────┐               ┌─────────────┐ │\n│   │         │  ── Request ──► │  Rate Limiting  │ ── Forward ─► │ Logic Apps  │ │\n│   │ Foundry │                 │  JWT Validation │               │ ServiceNow  │ │\n│   │  Agent  │  ◄── Response ─ │  Content Safety │ ◄── Response ─│ Salesforce  │ │\n│   │         │                 │  Token Metrics  │               │ SAP         │ │\n│   └─────────┘                 │  Semantic Cache │               └─────────────┘ │\n│                               └─────────────────┘                                │\n│                                                                                  │\n│   Benefits:                                                                      │\n│   • Centralized governance for all MCP tool calls                               │\n│   • Cost control via rate and token limiting                                     │\n│   • Security via JWT validation and content safety                               │\n│   • Observability via metrics and logging                                        │\n│   • Performance via semantic caching                                             │\n│                                                                                  │\n└─────────────────────────────────────────────────────────────────────────────────┘\n```\n\n### AI Gateway Policy Matrix\n\n| Policy | Purpose | Phase | Tier Support |\n|--------|---------|-------|--------------|\n| `validate-jwt` | Authentication | Inbound | All |\n| `rate-limit-by-key` | Request rate control | Inbound | All except Consumption |\n| `llm-token-limit` | Token consumption control | Inbound | All except Consumption |\n| `llm-content-safety` | Prompt injection protection | Inbound | All |\n| `llm-semantic-cache-lookup` | Return cached responses | Inbound | All except Self-hosted |\n| `llm-emit-token-metric` | Usage tracking | Outbound | All |\n| `llm-semantic-cache-store` | Cache responses | Outbound | All except Self-hosted |\n\n### APIM AI Gateway Features by Tier\n\n| Feature | Classic | V2 | Consumption | Self-hosted |\n|---------|---------|-----|-------------|-------------|\n| Pass-through MCP Server | ✔️ | ✔️ | ❌ | ✔️ |\n| Export REST API as MCP | ✔️ | ✔️ | ❌ | ✔️ |\n| A2A Agent Protocol | ❌ | ✔️ | ❌ | ❌ |\n| Semantic Caching | ✔️ | ✔️ | ✔️ | ❌ |\n| Token Limiting | ✔️ | ✔️ | ❌ | ✔️ |\n| Rate Limiting | ✔️ | ✔️ | ❌ | ✔️ |\n\n### Production Checklist\n\n- [ ] Configure APIM with MCP server backend\n- [ ] Set up JWT validation with Azure AD app registration\n- [ ] Configure rate limiting policy (requests per minute)\n- [ ] Configure token limiting policy (tokens per minute)\n- [ ] Enable content safety with blocklists\n- [ ] Enable semantic caching for cost optimization\n- [ ] Set up Azure Monitor for metrics dashboards\n- [ ] Test rate limiting returns `429` on burst\n- [ ] Verify content safety blocks injection attempts\n\n### Monitoring in Azure Monitor\n\n| Metric | Description | Dashboard Use |\n|--------|-------------|---------------|\n| `TokensConsumed` | Total tokens used | Cost tracking |\n| `RequestCount` | Total requests | Usage patterns |\n| `RateLimitedRequests` | Blocked by rate limit | Capacity planning |\n| `CacheHitRatio` | Semantic cache hits | Cost optimization |\n| `ContentSafetyBlocked` | Blocked by content safety | Security monitoring |\n\n---\n\n## Next Steps\n\n1. **Deploy APIM** in your subscription (Standard v2 or Premium v2 recommended)\n2. **Import your MCP server** as an API backend\n3. **Configure AI gateway policies** for governance\n4. **Set up Azure Monitor** dashboards for observability\n5. **Test with agents** in Azure AI Foundry Playground\n\nContinue to `../10-tool-catalog-registration-in-foundry` for tool catalog integration."
  },
  {
   "cell_type": "markdown",
   "id": "26qi3gd4zb0h",
   "source": "---\n\n<div align=\"center\">\n\n## License & Attribution\n\nThis notebook is part of the **Azure AI Foundry Demo Repository**\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](../LICENSE)\n\n**Original Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n\n**Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n\n---\n\n*If you use, modify, or distribute this work, you must provide appropriate credit to the original author as required by the [Apache License 2.0](../LICENSE).*\n\n**Copyright © 2025 Ozgur Guler. All rights reserved.**\n\n</div>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}