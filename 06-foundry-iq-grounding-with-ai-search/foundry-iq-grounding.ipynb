{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Foundry IQ: Grounding with Azure AI Search\n\n> **Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n> **Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n> **Copyright 2025 Ozgur Guler. All rights reserved.**\n\n---\n\n## What is Foundry IQ?\n\n**Foundry IQ** is Azure AI Foundry's intelligent retrieval system that connects AI agents to your proprietary data. It enables **Retrieval-Augmented Generation (RAG)** by automatically:\n\n1. **Intercepting user queries** before they reach the LLM\n2. **Searching your indexed content** in Azure AI Search\n3. **Injecting relevant context** into the LLM prompt\n4. **Generating grounded responses** based on your actual data\n\n### Why Grounding Matters\n\nWithout grounding, LLMs can only use their training data (which has a knowledge cutoff) and may \"hallucinate\" - generating plausible but incorrect information. Grounding solves this by:\n\n| Problem | Solution with Grounding |\n|---------|------------------------|\n| Knowledge cutoff | Access to your latest indexed documents |\n| Hallucination | Responses anchored to real retrieved content |\n| Generic answers | Domain-specific responses from your data |\n| No citations | Source attribution from search results |\n\n---\n\n## Architecture: What Happens Under the Hood\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              USER QUERY                                      │\n│                    \"What is the IMF's economic outlook?\"                     │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                      │\n                                      ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         AZURE AI FOUNDRY AGENT                               │\n│  ┌─────────────────────────────────────────────────────────────────────┐    │\n│  │  1. Agent receives query                                             │    │\n│  │  2. Recognizes AzureAISearchTool is available                       │    │\n│  │  3. Decides to call the tool (based on instructions)                │    │\n│  └─────────────────────────────────────────────────────────────────────┘    │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                      │\n                                      ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      AZURE AI SEARCH (Foundry IQ)                            │\n│  ┌─────────────────────────────────────────────────────────────────────┐    │\n│  │  4. Query is sent to AI Search index                                │    │\n│  │  5. Search executes (SIMPLE, SEMANTIC, or VECTOR)                   │    │\n│  │  6. Top-K relevant documents retrieved                              │    │\n│  │  7. Results returned with relevance scores                          │    │\n│  └─────────────────────────────────────────────────────────────────────┘    │\n│                                                                              │\n│  Index: imf_baseline                                                         │\n│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐                        │\n│  │ Doc 1    │ │ Doc 2    │ │ Doc 3    │ │ Doc N    │                        │\n│  │ Score:   │ │ Score:   │ │ Score:   │ │ Score:   │                        │\n│  │ 0.95     │ │ 0.87     │ │ 0.82     │ │ ...      │                        │\n│  └──────────┘ └──────────┘ └──────────┘ └──────────┘                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                      │\n                                      ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              LLM (gpt-5-nano)                                │\n│  ┌─────────────────────────────────────────────────────────────────────┐    │\n│  │  8. LLM receives:                                                    │    │\n│  │     - Original user query                                           │    │\n│  │     - Retrieved document chunks (grounding context)                 │    │\n│  │     - Agent instructions (cite sources, don't hallucinate)          │    │\n│  │                                                                      │    │\n│  │  9. LLM generates response grounded in retrieved content            │    │\n│  └─────────────────────────────────────────────────────────────────────┘    │\n└─────────────────────────────────────────────────────────────────────────────┘\n                                      │\n                                      ▼\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                           GROUNDED RESPONSE                                  │\n│  \"According to the IMF World Economic Outlook [Doc 1], the global           │\n│   economy is projected to grow at 3.2% in 2024...\"                          │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## Configuration\n\nWe'll connect to the following AI Search index:\n- **AI Search Service**: `chatops`\n- **Index Name**: `imf_baseline`\n\n## Prerequisites\n\n1. **Azure CLI authenticated**: Run `az login`\n2. **Azure AI Foundry project**: From previous sections\n3. **Azure AI Search service**: With an existing index\n4. **Connection configured**: Between Foundry project and AI Search service"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install azure-ai-projects --pre --quiet\n!pip install azure-identity python-dotenv requests --quiet\n\nprint(\"Packages installed\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment from parent directory\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Foundry Configuration\n",
    "FOUNDRY_ACCOUNT = os.getenv(\"FOUNDRY_ACCOUNT_NAME\", \"ozgurguler-7212-resource\")\n",
    "PROJECT_NAME = os.getenv(\"FOUNDRY_PROJECT_NAME\", \"ozgurguler-7212\")\n",
    "PROJECT_ENDPOINT = f\"https://{FOUNDRY_ACCOUNT}.services.ai.azure.com/api/projects/{PROJECT_NAME}\"\n",
    "\n",
    "# AI Search Configuration\n",
    "AI_SEARCH_SERVICE = os.getenv(\"AI_SEARCH_SERVICE\", \"chatops\")\n",
    "AI_SEARCH_ENDPOINT = f\"https://{AI_SEARCH_SERVICE}.search.windows.net\"\n",
    "AI_SEARCH_INDEX = os.getenv(\"AI_SEARCH_INDEX\", \"imf_baseline\")\n",
    "\n",
    "# Model Configuration\n",
    "CHAT_MODEL = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-5-nano\")\n",
    "\n",
    "# Agent Configuration\n",
    "AGENT_NAME = \"imf-grounded-agent\"\n",
    "\n",
    "print(f\"Project Endpoint: {PROJECT_ENDPOINT}\")\n",
    "print(f\"AI Search Endpoint: {AI_SEARCH_ENDPOINT}\")\n",
    "print(f\"AI Search Index: {AI_SEARCH_INDEX}\")\n",
    "print(f\"Model: {CHAT_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Initialize the client\n",
    "credential = DefaultAzureCredential()\n",
    "client = AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=credential)\n",
    "\n",
    "print(\"AIProjectClient initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Verify AI Search Connection\n",
    "\n",
    "First, let's check if a connection to the AI Search service exists in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Skip connection listing - go directly to getting the default AI Search connection\nprint(\"Skipping connection list (can be slow)...\")\nprint(\"Will get default AI Search connection in next cell.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "from azure.ai.projects.models import ConnectionType\n\n# Try to get the default AI Search connection\nprint(\"Getting default AI Search connection...\")\n\ntry:\n    ai_search_connection = client.connections.get_default(ConnectionType.AZURE_AI_SEARCH)\n    AI_SEARCH_CONNECTION_ID = ai_search_connection.id\n    \n    print(f\"\\nDefault AI Search connection found!\")\n    print(f\"  Name: {ai_search_connection.name}\")\n    print(f\"  ID: {ai_search_connection.id}\")\n    \nexcept Exception as e:\n    print(f\"\\nNo default AI Search connection: {e}\")\n    print(\"\\nUsing manual connection ID from earlier output...\")\n    \n    # Use the connection ID we saw earlier in the output\n    AI_SEARCH_CONNECTION_ID = \"/subscriptions/a20bc194-9787-44ee-9c7f-7c3130e651b6/resourceGroups/rg-ozgurguler-7212/providers/Microsoft.CognitiveServices/accounts/ozgurguler-7212-resource/projects/ozgurguler-7212/connections/chatopsozgulert2mx2h\"\n    print(f\"  Using: {AI_SEARCH_CONNECTION_ID}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Section 2b: Create AI Search Connection (if needed)\n",
    "\n",
    "If no connection exists, create one using the Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AI Search connection if not found\n",
    "CREATE_CONNECTION = False  # Set to True to create\n",
    "\n",
    "if CREATE_CONNECTION and AI_SEARCH_CONNECTION_ID is None:\n",
    "    print(\"Creating AI Search connection...\")\n",
    "    \n",
    "    try:\n",
    "        from azure.ai.ml import MLClient\n",
    "        from azure.ai.ml.entities import AzureAISearchConnection\n",
    "        \n",
    "        # Get ML client\n",
    "        ml_client = MLClient(\n",
    "            credential=credential,\n",
    "            subscription_id=os.getenv(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "            resource_group_name=os.getenv(\"AZURE_RESOURCE_GROUP\", \"rg-ozgurguler-7212\"),\n",
    "            workspace_name=PROJECT_NAME\n",
    "        )\n",
    "        \n",
    "        # Create connection (using AAD auth - no API key)\n",
    "        connection = AzureAISearchConnection(\n",
    "            name=f\"{AI_SEARCH_SERVICE}-connection\",\n",
    "            endpoint=AI_SEARCH_ENDPOINT,\n",
    "            api_key=None  # Use AAD authentication\n",
    "        )\n",
    "        \n",
    "        ml_client.connections.create_or_update(connection)\n",
    "        print(f\"✅ Created connection: {connection.name}\")\n",
    "        \n",
    "        # Refresh connection ID\n",
    "        ai_search_connection = client.connections.get_default(ConnectionType.AZURE_AI_SEARCH)\n",
    "        AI_SEARCH_CONNECTION_ID = ai_search_connection.id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating connection: {e}\")\n",
    "        print(\"\\nAlternatively, create the connection via Azure Portal:\")\n",
    "        print(\"1. Go to Azure AI Foundry portal\")\n",
    "        print(\"2. Select your project\")\n",
    "        print(\"3. Go to Settings > Connections\")\n",
    "        print(\"4. Add a new Azure AI Search connection\")\n",
    "else:\n",
    "    if AI_SEARCH_CONNECTION_ID:\n",
    "        print(\"AI Search connection already exists\")\n",
    "    else:\n",
    "        print(\"Connection creation skipped (CREATE_CONNECTION = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Verify the Search Index\n",
    "\n",
    "Let's verify the `imf_baseline` index exists and check its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Get AI Search service info\n",
    "print(f\"Checking index '{AI_SEARCH_INDEX}' on '{AI_SEARCH_SERVICE}'...\\n\")\n",
    "\n",
    "# Use REST API to check index (requires proper auth)\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    # Get token for AI Search\n",
    "    token_provider = get_bearer_token_provider(\n",
    "        DefaultAzureCredential(),\n",
    "        \"https://search.azure.com/.default\"\n",
    "    )\n",
    "    token = token_provider()\n",
    "    \n",
    "    # Check index exists\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(\n",
    "        f\"{AI_SEARCH_ENDPOINT}/indexes/{AI_SEARCH_INDEX}?api-version=2024-07-01\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        index_info = response.json()\n",
    "        print(f\"✅ Index '{AI_SEARCH_INDEX}' found!\\n\")\n",
    "        print(f\"Fields:\")\n",
    "        for field in index_info.get('fields', [])[:10]:  # Show first 10 fields\n",
    "            print(f\"  - {field['name']} ({field['type']})\")\n",
    "        if len(index_info.get('fields', [])) > 10:\n",
    "            print(f\"  ... and {len(index_info['fields']) - 10} more fields\")\n",
    "    else:\n",
    "        print(f\"⚠️  Index not found or access denied: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking index: {e}\")\n",
    "    print(\"\\nNote: You may need proper RBAC permissions on the AI Search service.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "---\n\n## Section 4: Configure the Azure AI Search Tool\n\n### How the Tool Works\n\nThe `AzureAISearchAgentTool` is a **function-calling tool** that the LLM can invoke during a conversation. When configured:\n\n1. **Tool Registration**: The tool definition is sent to the LLM as part of the system context\n2. **Tool Invocation**: When the LLM decides it needs information, it generates a tool call\n3. **Search Execution**: Foundry executes the search against your AI Search index\n4. **Result Injection**: Retrieved documents are added to the conversation context\n5. **Response Generation**: The LLM uses the retrieved content to formulate a response\n\n### Query Types Explained\n\n| Query Type | How It Works | Best For |\n|------------|--------------|----------|\n| `SIMPLE` | Keyword matching with BM25 ranking | Exact term searches, structured queries |\n| `SEMANTIC` | AI-powered understanding of meaning | Natural language questions, concept matching |\n| `VECTOR` | Embedding similarity search | Finding conceptually similar content |\n| `VECTOR_SIMPLE_HYBRID` | Combines keyword + vector | Balanced precision and recall |\n| `VECTOR_SEMANTIC_HYBRID` | Combines semantic + vector | Best overall relevance |\n\n### Tool Structure\n\n```python\nAzureAISearchAgentTool(\n    azure_ai_search=AzureAISearchToolResource(\n        indexes=[\n            AISearchIndexResource(\n                project_connection_id=\"...\",  # Connection to AI Search service\n                index_name=\"...\",              # Which index to search\n                query_type=\"SIMPLE\",           # How to search\n            )\n        ]\n    )\n)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "from azure.ai.projects.models import (\n    AzureAISearchAgentTool,\n    AzureAISearchToolResource,\n    AISearchIndexResource,\n    AzureAISearchQueryType,\n)\n\n# Configure the Azure AI Search tool\nprint(\"Configuring Azure AI Search tool...\\n\")\n\nif AI_SEARCH_CONNECTION_ID:\n    # Create the AI Search tool with the correct nested structure\n    ai_search_tool = AzureAISearchAgentTool(\n        azure_ai_search=AzureAISearchToolResource(\n            indexes=[\n                AISearchIndexResource(\n                    project_connection_id=AI_SEARCH_CONNECTION_ID,\n                    index_name=AI_SEARCH_INDEX,\n                    query_type=AzureAISearchQueryType.SIMPLE,\n                )\n            ]\n        )\n    )\n    \n    print(f\"AI Search tool configured:\")\n    print(f\"  Connection ID: {AI_SEARCH_CONNECTION_ID}\")\n    print(f\"  Index: {AI_SEARCH_INDEX}\")\n    print(f\"  Query Type: SIMPLE\")\nelse:\n    print(\"Cannot configure tool - no AI Search connection ID\")\n    print(\"Please create an AI Search connection first (Section 2b)\")\n    ai_search_tool = None"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "---\n\n## Section 5: Create Grounded Agent\n\n### Agent Definition Components\n\nWhen we create a grounded agent, we're combining several elements:\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    PromptAgentDefinition                         │\n├─────────────────────────────────────────────────────────────────┤\n│  model: \"gpt-5-nano\"                                            │\n│    └─ Which LLM processes queries and generates responses       │\n│                                                                  │\n│  instructions: \"You are a helpful assistant...\"                 │\n│    └─ System prompt that guides agent behavior                  │\n│    └─ CRITICAL: Include grounding rules here!                   │\n│                                                                  │\n│  tools: [AzureAISearchAgentTool(...)]                           │\n│    └─ Available tools the agent can invoke                      │\n│    └─ Agent decides WHEN to use tools based on query            │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### The Importance of Instructions\n\nThe agent instructions are crucial for effective grounding. They tell the LLM:\n\n1. **When to search**: \"You MUST use the Azure AI Search tool...\"\n2. **How to use results**: \"Ground your answers in the retrieved documents\"\n3. **How to handle failures**: \"If search returns no results, say...\"\n4. **Citation requirements**: \"Always cite your sources...\"\n\nWithout proper instructions, the LLM might:\n- Answer from its training data instead of searching\n- Ignore retrieved content\n- Fail to cite sources\n- Hallucinate when search returns no results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent instructions optimized for grounding\n",
    "GROUNDED_AGENT_INSTRUCTIONS = f\"\"\"\n",
    "You are a helpful assistant that answers questions using the IMF (International Monetary Fund) knowledge base.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. You MUST use the Azure AI Search tool to find relevant information before answering.\n",
    "2. You MUST ground your answers in the retrieved documents.\n",
    "3. If the search returns no relevant results, say \"I couldn't find information about that in the knowledge base.\"\n",
    "4. NEVER make up information - only use what you find in the search results.\n",
    "5. Always cite your sources by mentioning which document the information came from.\n",
    "\n",
    "The knowledge base contains: {AI_SEARCH_INDEX}\n",
    "\n",
    "When responding:\n",
    "- Be concise and accurate\n",
    "- Quote relevant passages when helpful\n",
    "- If asked about topics outside the knowledge base, politely redirect to what you can help with\n",
    "\"\"\"\n",
    "\n",
    "print(\"Agent Instructions:\")\n",
    "print(GROUNDED_AGENT_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "from azure.ai.projects.models import PromptAgentDefinition\n\n# Create the grounded agent\nprint(f\"Creating grounded agent: {AGENT_NAME}...\\n\")\n\nif ai_search_tool:\n    try:\n        # Check if agent already exists\n        try:\n            existing_agent = client.agents.get(agent_name=AGENT_NAME)\n            print(f\"Agent already exists: {existing_agent.name} (version: {existing_agent.version})\")\n            agent = existing_agent\n        except:\n            # Create new agent using PromptAgentDefinition\n            agent = client.agents.create_version(\n                agent_name=AGENT_NAME,\n                definition=PromptAgentDefinition(\n                    model=CHAT_MODEL,\n                    instructions=GROUNDED_AGENT_INSTRUCTIONS,\n                    tools=[ai_search_tool],  # Pass the AzureAISearchAgentTool\n                )\n            )\n            print(f\"Created grounded agent: {agent.name}\")\n            print(f\"   Version: {agent.version}\")\n            print(f\"   Model: {CHAT_MODEL}\")\n        \n    except Exception as e:\n        print(f\"Error creating agent: {e}\")\n        import traceback\n        traceback.print_exc()\n        agent = None\nelse:\n    print(\"Cannot create agent - AI Search tool not configured\")\n    agent = None"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "---\n\n## Section 6: Test the Grounded Agent\n\n### What Happens During Agent Invocation\n\nWhen you send a query to the grounded agent, here's the detailed flow:\n\n```\nStep 1: User sends query\n        \"What is the IMF's role in global economic stability?\"\n                              │\n                              ▼\nStep 2: Agent analyzes query + available tools\n        - Sees AzureAISearchTool is available\n        - Instructions say \"MUST use search tool\"\n        - Decides to invoke the tool\n                              │\n                              ▼\nStep 3: Tool call generated (internal)\n        {\n          \"tool\": \"azure_ai_search\",\n          \"query\": \"IMF role global economic stability\",\n          \"index\": \"imf_baseline\"\n        }\n                              │\n                              ▼\nStep 4: Foundry executes search\n        - Connects to AI Search via connection ID\n        - Runs query against imf_baseline index\n        - Retrieves top-K documents (default: 5)\n                              │\n                              ▼\nStep 5: Results injected into context\n        [Retrieved Document 1]: \"The IMF promotes international...\"\n        [Retrieved Document 2]: \"Key functions include surveillance...\"\n        [Retrieved Document 3]: \"The IMF provides financial assistance...\"\n                              │\n                              ▼\nStep 6: LLM generates grounded response\n        - Uses retrieved documents as primary source\n        - Follows instructions to cite sources\n        - Avoids hallucination by sticking to retrieved content\n                              │\n                              ▼\nStep 7: Response returned to user\n        \"According to the IMF documentation, the organization plays\n         several key roles in global economic stability: [1] promoting\n         international monetary cooperation, [2] providing surveillance\n         of economic policies...\"\n```\n\n### Understanding the Response\n\nThe agent's response should:\n- **Reference retrieved content**: Quotes or paraphrases from indexed documents\n- **Include citations**: Indicates which documents provided the information\n- **Acknowledge limitations**: Says \"I couldn't find...\" if search returns nothing\n- **Stay on topic**: Redirects questions outside the knowledge base scope"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Get OpenAI client for agent invocation\nopenai_client = client.get_openai_client()\n\ndef ask_grounded_agent(question: str, agent_name: str) -> str:\n    \"\"\"Send a question to the grounded agent and get a response.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Question: {question}\")\n    print(\"=\"*60)\n    \n    try:\n        # Create a conversation\n        conversation = openai_client.conversations.create()\n        \n        # Send the message with agent reference\n        response = openai_client.responses.create(\n            input=question,\n            conversation=conversation.id,\n            extra_body={\"agent\": {\"name\": agent_name, \"type\": \"agent_reference\"}},\n        )\n        \n        print(f\"\\nStatus: {response.status}\")\n        print(f\"\\nAgent Response:\\n{response.output_text}\")\n        return response.output_text\n        \n    except Exception as e:\n        print(f\"\\nError: {e}\")\n        return None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Test questions for the IMF knowledge base\nif agent:\n    test_questions = [\n        \"What is the IMF's role in global economic stability?\",\n        \"Summarize the key points from the latest available IMF report.\",\n        \"What recommendations does the IMF provide for developing economies?\",\n    ]\n    \n    for question in test_questions:\n        response = ask_grounded_agent(question, agent.name)\n        print(\"\\n\")\nelse:\n    print(\"Agent not available - please create it first\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Alternative - Using Prompt Agent with AI Search\n",
    "\n",
    "You can also use the newer `PromptAgentDefinition` with the AI Search tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# This section is now redundant since we're using the correct API above\n# The agent was already created in cell-16 with the proper AzureAISearchAgentTool\n\nprint(\"Agent already created in Section 5 above\")\nprint(f\"Agent: {agent.name if agent else 'Not created'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prompt agent using conversations API\n",
    "if prompt_agent:\n",
    "    openai_client = client.get_openai_client()\n",
    "    \n",
    "    # Create conversation\n",
    "    conversation = openai_client.conversations.create()\n",
    "    print(f\"Created conversation: {conversation.id}\\n\")\n",
    "    \n",
    "    # Test query\n",
    "    test_query = \"What are the main economic indicators discussed in the IMF baseline?\"\n",
    "    print(f\"Query: {test_query}\")\n",
    "    \n",
    "    response = openai_client.responses.create(\n",
    "        input=test_query,\n",
    "        conversation=conversation.id,\n",
    "        extra_body={\"agent\": {\"name\": prompt_agent.name, \"type\": \"agent_reference\"}},\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStatus: {response.status}\")\n",
    "    print(f\"\\nAgent Response:\\n{response.output_text}\")\n",
    "else:\n",
    "    print(\"Prompt agent not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete agents\n",
    "DELETE_AGENTS = False  # Set to True to delete\n",
    "\n",
    "if DELETE_AGENTS:\n",
    "    try:\n",
    "        if agent:\n",
    "            client.agents.delete_agent(agent.id)\n",
    "            print(f\"Deleted agent: {agent.id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting agent: {e}\")\n",
    "    \n",
    "    try:\n",
    "        if prompt_agent:\n",
    "            client.agents.delete(agent_name=PROMPT_AGENT_NAME)\n",
    "            print(f\"Deleted prompt agent: {PROMPT_AGENT_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting prompt agent: {e}\")\n",
    "else:\n",
    "    print(\"Agent deletion skipped (DELETE_AGENTS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### What is Foundry IQ Doing?\n\n**Foundry IQ** orchestrates the entire RAG (Retrieval-Augmented Generation) pipeline:\n\n```\n┌────────────────────────────────────────────────────────────────────────────┐\n│                        FOUNDRY IQ ORCHESTRATION                             │\n├────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│   ┌─────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐   │\n│   │  User   │───▶│    Agent     │───▶│  AI Search  │───▶│     LLM     │   │\n│   │  Query  │    │  (Tool Call) │    │  (Retrieve) │    │  (Generate) │   │\n│   └─────────┘    └──────────────┘    └─────────────┘    └─────────────┘   │\n│                                                                             │\n│   Key Operations:                                                           │\n│   • Query Understanding    - Agent decides what to search for              │\n│   • Search Execution       - Foundry IQ runs the search                    │\n│   • Context Augmentation   - Results injected into LLM context             │\n│   • Response Generation    - LLM produces grounded answer                  │\n│   • Citation Tracking      - Source attribution maintained                 │\n│                                                                             │\n└────────────────────────────────────────────────────────────────────────────┘\n```\n\n### What We Built\n\n1. **Connected to AI Search** - Established secure connection via `project_connection_id`\n2. **Configured Search Tool** - Set up `AzureAISearchAgentTool` with query parameters\n3. **Created Grounded Agent** - Combined LLM + tool + instructions\n4. **Tested RAG Pipeline** - Verified end-to-end grounding with real queries\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Grounding** | Anchoring LLM responses in retrieved content from your data |\n| **RAG** | Retrieval-Augmented Generation - search then generate pattern |\n| **Tool Calling** | LLM decides when to invoke search based on query |\n| **Connection** | Secure link between Foundry project and AI Search service |\n| **Query Type** | Search algorithm: SIMPLE (keyword), SEMANTIC (AI), VECTOR (embeddings) |\n| **Top-K** | Number of documents retrieved per search (default: 5) |\n\n### Under the Hood: Data Flow\n\n| Stage | Component | What Happens |\n|-------|-----------|--------------|\n| 1. Input | User Query | Natural language question received |\n| 2. Planning | Agent/LLM | Decides to use search tool |\n| 3. Retrieval | AI Search | Executes query, returns ranked documents |\n| 4. Augmentation | Foundry IQ | Injects documents into LLM context |\n| 5. Generation | LLM | Produces response using retrieved content |\n| 6. Output | Response | Grounded answer with citations |\n\n### Best Practices\n\n| Practice | Why It Matters |\n|----------|---------------|\n| Use SEMANTIC query type | Better understanding of natural language |\n| Set low temperature (0.1-0.3) | More factual, less creative responses |\n| Include citation instructions | Traceability and trust |\n| Add \"I don't know\" fallback | Prevents hallucination on empty results |\n| One index per agent | Simpler reasoning, better focus |\n\n### Troubleshooting\n\n| Issue | Likely Cause | Solution |\n|-------|--------------|----------|\n| No connection found | Missing project connection | Create via Portal or SDK |\n| Empty responses | Index has no matching content | Check index data, broaden query |\n| Hallucination | Weak instructions | Add stricter grounding rules |\n| Wrong citations | Field mapping issues | Verify index schema |\n| Slow responses | Large index, complex queries | Use filters, optimize index |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to `../07-logic-apps-as-mcp-server` for integration with Logic Apps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36zgt4cyqby",
   "source": "---\n\n<div align=\"center\">\n\n## License & Attribution\n\nThis notebook is part of the **Azure AI Foundry Demo Repository**\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](../LICENSE)\n\n**Original Author:** Ozgur Guler | AI Solution Leader, AI Innovation Hub\n\n**Contact:** [ozgur.guler1@gmail.com](mailto:ozgur.guler1@gmail.com)\n\n---\n\n*If you use, modify, or distribute this work, you must provide appropriate credit to the original author as required by the [Apache License 2.0](../LICENSE).*\n\n**Copyright © 2025 Ozgur Guler. All rights reserved.**\n\n</div>",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}